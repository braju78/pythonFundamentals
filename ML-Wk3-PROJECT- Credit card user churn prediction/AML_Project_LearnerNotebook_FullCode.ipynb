{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a54fa0f"
      },
      "source": [
        "# Credit Card Users Churn Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EaJ8AGwpM-2"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3-QehJxbp0t"
      },
      "source": [
        "### Business Context\n",
        "\n",
        "The Thera bank recently saw a steep decline in the number of users of their credit card, credit cards are a good source of income for banks because of different kinds of fees charged by the banks like annual fees, balance transfer fees, and cash advance fees, late payment fees, foreign transaction fees, and others. Some fees are charged to every user irrespective of usage, while others are charged under specified circumstances.\n",
        "\n",
        "Customers’ leaving credit cards services would lead bank to loss, so the bank wants to analyze the data of customers and identify the customers who will leave their credit card services and reason for same – so that bank could improve upon those areas\n",
        "\n",
        "You as a Data scientist at Thera bank need to come up with a classification model that will help the bank improve its services so that customers do not renounce their credit cards\n",
        "\n",
        "### Data Description\n",
        "\n",
        "* CLIENTNUM: Client number. Unique identifier for the customer holding the account\n",
        "* Attrition_Flag: Internal event (customer activity) variable - if the account is closed then \"Attrited Customer\" else \"Existing Customer\"\n",
        "* Customer_Age: Age in Years\n",
        "* Gender: Gender of the account holder\n",
        "* Dependent_count: Number of dependents\n",
        "* Education_Level: Educational Qualification of the account holder - Graduate, High School, Unknown, Uneducated, College(refers to college student), Post-Graduate, Doctorate\n",
        "* Marital_Status: Marital Status of the account holder\n",
        "* Income_Category: Annual Income Category of the account holder\n",
        "* Card_Category: Type of Card\n",
        "* Months_on_book: Period of relationship with the bank (in months)\n",
        "* Total_Relationship_Count: Total no. of products held by the customer\n",
        "* Months_Inactive_12_mon: No. of months inactive in the last 12 months\n",
        "* Contacts_Count_12_mon: No. of Contacts in the last 12 months\n",
        "* Credit_Limit: Credit Limit on the Credit Card\n",
        "* Total_Revolving_Bal: Total Revolving Balance on the Credit Card\n",
        "* Avg_Open_To_Buy: Open to Buy Credit Line (Average of last 12 months)\n",
        "* Total_Amt_Chng_Q4_Q1: Change in Transaction Amount (Q4 over Q1)\n",
        "* Total_Trans_Amt: Total Transaction Amount (Last 12 months)\n",
        "* Total_Trans_Ct: Total Transaction Count (Last 12 months)\n",
        "* Total_Ct_Chng_Q4_Q1: Change in Transaction Count (Q4 over Q1)\n",
        "* Avg_Utilization_Ratio: Average Card Utilization Ratio\n",
        "\n",
        "#### What Is a Revolving Balance?\n",
        "\n",
        "- If we don't pay the balance of the revolving credit account in full every month, the unpaid portion carries over to the next month. That's called a revolving balance\n",
        "\n",
        "\n",
        "##### What is the Average Open to buy?\n",
        "\n",
        "- 'Open to Buy' means the amount left on your credit card to use. Now, this column represents the average of this value for the last 12 months.\n",
        "\n",
        "##### What is the Average utilization Ratio?\n",
        "\n",
        "- The Avg_Utilization_Ratio represents how much of the available credit the customer spent. This is useful for calculating credit scores.\n",
        "\n",
        "\n",
        "##### Relation b/w Avg_Open_To_Buy, Credit_Limit and Avg_Utilization_Ratio:\n",
        "\n",
        "- ( Avg_Open_To_Buy / Credit_Limit ) + Avg_Utilization_Ratio = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbHOIdlwcrqR"
      },
      "source": [
        "### **Please read the instructions carefully before starting the project.**\n",
        "This is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "* Blanks '_______' are provided in the notebook that\n",
        "needs to be filled with an appropriate code to get the correct result. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space.\n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\". Running incomplete code may throw error.\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_-uuGqH-qTt"
      },
      "source": [
        "## Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "83D17_Wl4jal"
      },
      "outputs": [],
      "source": [
        "# Installing the libraries with the specified version.\n",
        "# uncomment and run the following line if Google Colab is being used\n",
        "!pip install scikit-learn==1.2.2 seaborn==0.13.1 matplotlib==3.7.1 numpy==1.25.2 pandas==1.5.3 imbalanced-learn==0.10.1 xgboost==2.0.3 -q --user"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the libraries with the specified version.\n",
        "# uncomment and run the following lines if Jupyter Notebook is being used\n",
        "# !pip install scikit-learn==1.2.2 seaborn==0.13.1 matplotlib==3.7.1 numpy==1.25.2 pandas==1.5.3 imblearn==0.12.0 xgboost==2.0.3 -q --user\n",
        "# !pip install --upgrade -q threadpoolctl"
      ],
      "metadata": {
        "id": "XVB5DgmofkGQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again*."
      ],
      "metadata": {
        "id": "G3tBAjBgrLAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries to help with reading and manipulating data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# To suppress scientific notations\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
        "\n",
        "# Libaries to help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To tune model, get different metric scores, and split data\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "\n",
        "# To be used for data scaling and one hot encoding\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "\n",
        "# To impute missing values\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# To oversample and undersample data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# To do hyperparameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# To define maximum number of columns to be displayed in a dataframe\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "# To supress scientific notations for a dataframe\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
        "\n",
        "# To help with model building\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    RandomForestClassifier,\n",
        "    BaggingClassifier,\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# To supress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "8I-FQB2qrNmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxhpZv9y-qTw"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJnKoHy14jam"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('BankChurners.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvpMDcaaMKtI"
      },
      "source": [
        "## Data Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01hJQ7EfMKtK"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make a copy and work on the copy, so the original data (df) is intact.\n",
        "data = df.copy()"
      ],
      "metadata": {
        "id": "X4aB-cfmz2QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Review top and bottom 5 rows to understand the data.\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ywRFEBr30CVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "0Q-zamcq0Uea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "uitdya-l2PQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are few non-numeric columns (features) that need to be worked on."
      ],
      "metadata": {
        "id": "tirGH3MS2owR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check for duplicate datasets\n",
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "BOfkn6on2aEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No duplicate datasets are found, hence no clean up is needed."
      ],
      "metadata": {
        "id": "DNvdQK1y2iCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check for missing values\n",
        "\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "OCC-lLVw2vkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Columns (features) Education_level and Marital_status are missing data. Need to replace with proper values soon."
      ],
      "metadata": {
        "id": "5uiK0EZN23d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe(include='all').T"
      ],
      "metadata": {
        "id": "c4g5-r6c4J97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify unique values and the number of occurences for all the categorical variables.\n",
        "for i in data.describe(include=[\"object\"]).columns:\n",
        "    print(\"Unique values in\", i, \"are :\")\n",
        "    print(data[i].value_counts())\n",
        "    print(\"*\" * 50)"
      ],
      "metadata": {
        "id": "0xGLeFCu8k5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLIENTNUM consists of uniques ID for clients and hence will not add value to the modeling\n",
        "data.drop([\"CLIENTNUM\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "TzNzJjfO8saK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Encoding the label (target variable).\n",
        "data[\"Attrition_Flag\"].replace(\"Existing Customer\", 0, inplace=True)\n",
        "data[\"Attrition_Flag\"].replace(\"Attrited Customer\", 1, inplace=True)\n",
        "\n",
        "data.tail(10)"
      ],
      "metadata": {
        "id": "w3glL5GU8yWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Retention_percentage = data['Attrition_Flag'].value_counts(normalize=True) * 100\n",
        "Retention_percentage"
      ],
      "metadata": {
        "id": "OhwkoTcF-60g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Observations:\n",
        "Total dataset size is 10k+, which is a decent size.\n",
        "Education_level and Marital_status features have many missing values. If these features are important, then null values need to be replaced with either high frequent or mean values.\n",
        "The target value data sets are unbalanced. The model may be biased towards high frequency outcomes, hence adjust the dataset.\n"
      ],
      "metadata": {
        "id": "rBAX6VgH-TGB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-yGG8LNSSMa"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bGVKmh75ri8"
      },
      "source": [
        "- EDA is an important part of any project involving data.\n",
        "- It is important to investigate and understand the data better before building a model with it.\n",
        "- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.\n",
        "- A thorough analysis of the data, in addition to the questions mentioned below, should be done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEyqzdJBb0jU"
      },
      "source": [
        "**Questions**:\n",
        "Based on the EDA, I have come up with the below answers.\n",
        "1. How is the total transaction amount distributed?\n",
        "Total_Trans_Amt is right skewed with outliers. Most of the customers have total_trans_amount $5000 or less.\n",
        "2. What is the distribution of the level of education of customers?\n",
        "Majority of the customer completed Graduation or High School. Very few customers have Post-Graduation or Doctorate\n",
        "3. What is the distribution of the level of income of customers?\n",
        "Most of the customer have income less than $40k (3561) and between 40k - 60k (1790). There are very few customers with income higher than $120k (727). Surprisingly 1112 customer's income is \"abc\" which seems to be an invalid income. Need to fix these values.\n",
        "4. How does the change in transaction amount between Q4 and Q1 (`total_ct_change_Q4_Q1`) vary by the customer's account status (`Attrition_Flag`)?\n",
        "As the total_ct_change_Q4_Q1 decreases, the likelihood of attrition increases, showing a negative correlation of -0.37. The average value of total_ct_change_Q4_Q1 for customers who have attrited is 0.5, whereas for existing customers, it is 0.7.\n",
        "5. How does the number of months a customer was inactive in the last 12 months (`Months_Inactive_12_mon`) vary by the customer's account status (`Attrition_Flag`)?\n",
        "As the value of Months_Inactive_12_mon increases, the likelihood of attrition also rises, exhibiting a positive correlation of 0.15. The average value of Months_Inactive_12_mon for customers who have attrited is 2, while the average for existing customers is 2.5.=\n",
        "6. What are the attributes that have a strong correlation with each other?\n",
        "a. Total_trans_ct exhibits a strong positive correlation with total_trans_amt, with a correlation coefficient of 0.81.\n",
        "b. Customer_age shows a strong positive correlation with months_on_book, with a correlation coefficient of 0.79.\n",
        "c. Avg_utilization_ratio has a strong positive correlation with total_revolving_balance, with a correlation coefficient of 0.62.\n",
        "d. Total_amt_chng_Q4_Q1 demonstrates a good positive correlation with total_ct_chng_Q4_Q1, with a correlation coefficient of 0.38.\n",
        "e. Avg_open_to_buy reflects a strong negative correlation with avg_utilization_ratio, with a correlation coefficient of -0.54.\n",
        "f. Credit_limit shows a strong negative correlation with avg_utilization_ratio, with a correlation coefficient of -0.48.\n",
        "g. Total_trans_ct has a good negative correlation with attrition_flag, with a correlation coefficient of -0.37.\n",
        "h. Total_trans_amt exhibits a good negative correlation with total_relationship_ct, with a correlation coefficient of -0.35.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YyWJgFlKlWM"
      },
      "source": [
        "#### The below functions need to be defined to carry out the Exploratory Data Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIP4bI3Zbp07"
      },
      "outputs": [],
      "source": [
        "# function to plot a boxplot and a histogram along the same scale.\n",
        "\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5021de33"
      },
      "outputs": [],
      "source": [
        "# function to create labeled barplots\n",
        "\n",
        "\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 1, 5))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 1, 5))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n].sort_values(),\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c08fe5b8"
      },
      "outputs": [],
      "source": [
        "# function to plot stacked bar chart\n",
        "\n",
        "def stacked_barplot(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[predictor].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 1, 5))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\", frameon=False,\n",
        "    )\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e90985c5"
      },
      "outputs": [],
      "source": [
        "### Function to plot distributions\n",
        "\n",
        "def distribution_plot_wrt_target(data, predictor, target):\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    target_uniq = data[target].unique()\n",
        "\n",
        "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[0]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 0],\n",
        "        color=\"teal\",\n",
        "    )\n",
        "\n",
        "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[1]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 1],\n",
        "        color=\"orange\",\n",
        "    )\n",
        "\n",
        "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
        "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
        "\n",
        "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
        "    sns.boxplot(\n",
        "        data=data,\n",
        "        x=target,\n",
        "        y=predictor,\n",
        "        ax=axs[1, 1],\n",
        "        showfliers=False,\n",
        "        palette=\"gist_rainbow\",\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Univariate analysis"
      ],
      "metadata": {
        "id": "hxauWC3xJA-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "continuous_columns = ['Customer_Age', 'Months_on_book', 'Credit_Limit', 'Total_Revolving_Bal', 'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt','Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\n",
        "for column in continuous_columns:\n",
        "  histogram_boxplot(data, column)\n",
        "\n",
        "discrete_columns = ['Gender','Dependent_count', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category', 'Total_Relationship_Count', 'Months_Inactive_12_mon','Contacts_Count_12_mon']\n",
        "for column in discrete_columns:\n",
        "  labeled_barplot(data, column)"
      ],
      "metadata": {
        "id": "3VQhzchtEFUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "numerical_data = data.select_dtypes(include=np.number)\n",
        "\n",
        "# Create correlation matrix\n",
        "correlation_matrix = numerical_data.corr()\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vuB65GcfH8W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bivariate Analysis"
      ],
      "metadata": {
        "id": "4gxxyXk4IzYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot stacked_barplot using the above function for all valid columns\n",
        "\n",
        "for column in discrete_columns:\n",
        "  stacked_barplot(data, column, 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "R468RojvIdBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the distribution_plot_wrt_target for all valid columns using the function mentioned above.\n",
        "\n",
        "for column in numerical_data:\n",
        "  distribution_plot_wrt_target(data, column, 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "Cij7yVpcJrnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knk0w9XH4jao"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JbJc1bX4jao"
      },
      "outputs": [],
      "source": [
        "# Detect the percentage of outliers for all continuous_columns\n",
        "\n",
        "def detect_outliers_percentage(data, continuous_columns):\n",
        "  \"\"\"\n",
        "  Detects the percentage of outliers for all continuous columns in a DataFrame.\n",
        "\n",
        "  Args:\n",
        "    data: The DataFrame containing the data.\n",
        "    continuous_columns: A list of column names representing continuous variables.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary where keys are column names and values are the percentage of outliers.\n",
        "  \"\"\"\n",
        "\n",
        "  outlier_percentages = {}\n",
        "  for column in continuous_columns:\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    num_outliers = len(data[(data[column] < lower_bound) | (data[column] > upper_bound)])\n",
        "    outlier_percentage = (num_outliers / len(data)) * 100\n",
        "\n",
        "    outlier_percentages[column] = outlier_percentage\n",
        "\n",
        "  return outlier_percentages\n",
        "\n",
        "\n",
        "# Example usage (assuming you have the 'data' DataFrame and 'continuous_columns' list defined)\n",
        "outlier_percentages = detect_outliers_percentage(data, continuous_columns)\n",
        "outlier_percentages"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Observations\n",
        "- Since the outlier data appears to be pertinent to this scenario, I will retain it in my analysis.\n",
        "\n",
        "- To prevent any leakage of my test data, I will first split the dataset before proceeding with preprocessing or imputation."
      ],
      "metadata": {
        "id": "cI547pHCKpe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into train, test and validation on a new copy\n",
        "# Using startify as the data is not balanced wrt the target variable\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into train, test, and validation sets\n",
        "X = data.drop('Attrition_Flag', axis=1)\n",
        "y = data['Attrition_Flag']\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=1\n",
        ")\n",
        "\n",
        "X_test, X_val, y_test, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=1\n",
        ")\n",
        "\n",
        "print(\"Train set shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape, y_test.shape)\n",
        "print(\"Validation set shape:\", X_val.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "EZBVNTl1LAzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the value counts of the income_category in all test, validation, and training datasets.\n",
        "\n",
        "print(\"Income Category Value Counts in Train Data:\")\n",
        "print(X_train['Income_Category'].value_counts())\n",
        "print(\"\\nIncome Category Value Counts in Validation Data:\")\n",
        "print(X_val['Income_Category'].value_counts())\n",
        "print(\"\\nIncome Category Value Counts in Test Data:\")\n",
        "print(X_test['Income_Category'].value_counts())"
      ],
      "metadata": {
        "id": "EU3oUS0FLFZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J99-7Kubp09"
      },
      "source": [
        "## Missing value imputation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hke9uYOfBqoQ"
      },
      "outputs": [],
      "source": [
        "# Replace 'Income_Category' values containing 'abc' with NaN\n",
        "X_train['Income_Category'].replace(\"abc\", np.nan, inplace=True)\n",
        "X_test['Income_Category'].replace(\"abc\", np.nan, inplace=True)\n",
        "X_val['Income_Category'].replace(\"abc\", np.nan, inplace=True)\n",
        "\n",
        "print(X_train.isna().sum())\n",
        "print('-------')\n",
        "print(X_test.isna().sum())\n",
        "print('-------')\n",
        "print(X_val.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the value counts of the income_category across all test, validation, and training datasets.\n",
        "\n",
        "print(\"Income Category Value Counts in Train Data:\")\n",
        "print(X_train['Income_Category'].value_counts())\n",
        "print(\"\\nIncome Category Value Counts in Validation Data:\")\n",
        "print(X_val['Income_Category'].value_counts())\n",
        "print(\"\\nIncome Category Value Counts in Test Data:\")\n",
        "X_test['Income_Category'].value_counts()"
      ],
      "metadata": {
        "id": "xcuky_9-McE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute the null values with simpleimputer\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create a SimpleImputer object with the 'most_frequent' strategy\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Fit and transform the imputer on the training data for categorical columns\n",
        "categorical_cols_with_null = ['Education_Level', 'Marital_Status', 'Income_Category']\n",
        "X_train[categorical_cols_with_null] = imputer.fit_transform(X_train[categorical_cols_with_null])\n",
        "X_test[categorical_cols_with_null] = imputer.transform(X_test[categorical_cols_with_null])\n",
        "X_val[categorical_cols_with_null] = imputer.transform(X_val[categorical_cols_with_null])\n",
        "\n",
        "print(X_train.isna().sum())\n",
        "print('-------')\n",
        "print(X_test.isna().sum())\n",
        "print('-------')\n",
        "print(X_val.isna().sum())"
      ],
      "metadata": {
        "id": "1uiZXtwaM3QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy variables for categorical features in train, test, and validation sets\n",
        "X_train = pd.get_dummies(X_train, drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, drop_first=True)\n",
        "X_val = pd.get_dummies(X_val, drop_first=True)\n",
        "\n",
        "print(\"Train set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)\n",
        "print(\"Validation set shape:\", X_val.shape)\n",
        "\n",
        "# Ensure all datasets have the same columns after creating dummy variables\n",
        "for column in X_train.columns:\n",
        "  if column not in X_test.columns:\n",
        "    print(column + \"ERROR :  not found in X_test\")\n",
        "  if column not in X_val.columns:\n",
        "    print(column + \"ERROR : not found in X_val\")\n",
        "\n",
        "# Reorder columns in test and validation sets to match the order of columns in train set\n",
        "X_test = X_test[X_train.columns]\n",
        "X_val = X_val[X_train.columns]"
      ],
      "metadata": {
        "id": "m4z9eAg1M_jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzOa9FGA6WtG"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZqmoqz7bp0-"
      },
      "source": [
        "### Model evaluation criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ORUgmUjDZC"
      },
      "source": [
        "The nature of predictions made by the classification model will translate as follows:\n",
        "\n",
        "- True positives (TP) are failures correctly predicted by the model.\n",
        "- False negatives (FN) are real failures in a generator where there is no detection by model.\n",
        "- False positives (FP) are failure detections in a generator where there is no failure.\n",
        "\n",
        "**Which metric to optimize?**\n",
        "\n",
        "* We need to choose the metric which will ensure that the maximum number of generator failures are predicted correctly by the model.\n",
        "* We would want Recall to be maximized as greater the Recall, the higher the chances of minimizing false negatives.\n",
        "* We want to minimize false negatives because if a model predicts that a machine will have no failure when there will be a failure, it will increase the maintenance cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQTqGKU4jap"
      },
      "source": [
        "**Let's define a function to output different metrics (including recall) on the train and test set and a function to show confusion matrix so that we do not have to use the same code repetitively while evaluating models.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIekBxwp4jaq"
      },
      "outputs": [],
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
        "def model_performance_classification_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    pred = model.predict(predictors)\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred)  # to compute Recall\n",
        "    precision = precision_score(target, pred)  # to compute Precision\n",
        "    f1 = f1_score(target, pred)  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\n",
        "            \"Accuracy\": acc,\n",
        "            \"Recall\": recall,\n",
        "            \"Precision\": precision,\n",
        "            \"F1\": f1\n",
        "\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqCDCbcw4jas"
      },
      "source": [
        "### Model Building with original data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBtuhurlhKyp"
      },
      "source": [
        "Sample code for model building with original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-tpzI7g4jas"
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "model_performance_df = pd.DataFrame(columns=['Model', 'Train Recall', 'Validation Recall'])\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n",
        "models.append((\"Decision Tree\", DecisionTreeClassifier(random_state=1)))\n",
        "models.append((\"AdaBoost\", AdaBoostClassifier(random_state=1)))\n",
        "models.append((\"Gradient Boosting\", GradientBoostingClassifier(random_state=1)))\n",
        "models.append((\"XG Boosting\", XGBClassifier(random_state=1)))\n",
        "\n",
        "print(\"\\n\" \"Model Performance:\" \"\\n\")\n",
        "for name, model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    scores = recall_score(y_train, model.predict(X_train))\n",
        "    scores_val = recall_score(y_val, model.predict(X_val))\n",
        "    print(\"Train Score: {}: {}\".format(name, scores))\n",
        "    print(\"Val Score: {}: {}\".format(name, scores_val))\n",
        "    # Create a temporary DataFrame for the new row\n",
        "    new_row_df = pd.DataFrame({'Model': [model] , 'ModelName' :[name +' Original Sampling']  , 'Train Recall': [scores], 'Validation Recall': [scores_val]})\n",
        "\n",
        "    # Concatenate the new row with the existing DataFrame\n",
        "    model_performance_df = pd.concat([model_performance_df, new_row_df], ignore_index=True)\n",
        "\n",
        "model_performance_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBKJaFU24jas"
      },
      "source": [
        "### Model Building with Oversampled data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKxnygkE4jat"
      },
      "outputs": [],
      "source": [
        "# Synthetic Minority Over Sampling Technique\n",
        "\n",
        "sm = SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)\n",
        "\n",
        "X_train_over, y_train_over = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Before Oversampling value counts of y_train: \" + str(y_train.value_counts()))\n",
        "\n",
        "print(\"\\nAfter Oversampling value counts of y_train: \" + str(y_train_over.value_counts()))\n",
        "\n",
        "print(\"Before Oversampling shape: \" + str(y_train.shape))\n",
        "\n",
        "print(\"\\nAfter Oversampling shape: \" + str(y_train_over.shape))\n",
        "X_train_over, y_train_over = sm.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYDlbnUO4jat"
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n",
        "models.append((\"Decision Tree\", DecisionTreeClassifier(random_state=1)))\n",
        "models.append((\"AdaBoost\", AdaBoostClassifier(random_state=1)))\n",
        "models.append((\"Gradient Boosting\", GradientBoostingClassifier(random_state=1)))\n",
        "models.append((\"XG Boosting\", XGBClassifier(random_state=1)))\n",
        "\n",
        "print(\"\\n\" \"Model Performance:\" \"\\n\")\n",
        "for name, model in models:\n",
        "    model.fit(X_train_over, y_train_over)\n",
        "    scores = recall_score(y_train_over, model.predict(X_train_over))\n",
        "    scores_val = recall_score(y_val, model.predict(X_val))\n",
        "    print(\"Train Over Score: {}: {}\".format(name, scores))\n",
        "    print(\"Val Over Score: {}: {}\".format(name, scores_val))\n",
        "    # Create a temporary DataFrame for the new row\n",
        "    new_row_df = pd.DataFrame({'Model': [model] , 'ModelName' :[name +' overSampling'] , 'Train Recall': [scores], 'Validation Recall': [scores_val]})\n",
        "\n",
        "    # Concatenate the new row with the existing DataFrame\n",
        "    model_performance_df = pd.concat([model_performance_df, new_row_df], ignore_index=True)\n",
        "\n",
        "model_performance_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aimb6bn4jat"
      },
      "source": [
        "### Model Building with Undersampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhxfTkvu4jat"
      },
      "outputs": [],
      "source": [
        "# Random undersampler for under sampling the data\n",
        "rus = RandomUnderSampler(random_state=1, sampling_strategy=1)\n",
        "\n",
        "X_train_un, y_train_un = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Before Undersampling value counts of y_train: \" + str(y_train.value_counts()))\n",
        "\n",
        "print(\"\\nAfter Undersampling value counts of y_train: \" + str(y_train_un.value_counts()))\n",
        "\n",
        "print(\"Before Undersampling shape: \" + str(y_train.shape))\n",
        "\n",
        "print(\"\\nAfter Undersampling shape: \" + str(y_train_un.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jROP_DVF4jau"
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n",
        "models.append((\"Decision Tree\", DecisionTreeClassifier(random_state=1)))\n",
        "models.append((\"AdaBoost\", AdaBoostClassifier(random_state=1)))\n",
        "models.append((\"Gradient Boosting\", GradientBoostingClassifier(random_state=1)))\n",
        "models.append((\"XG Boosting\", XGBClassifier(random_state=1)))\n",
        "\n",
        "print(\"\\n\" \"Model Performance:\" \"\\n\")\n",
        "for name, model in models:\n",
        "    model.fit(X_train_un, y_train_un)\n",
        "    scores = recall_score(y_train_un, model.predict(X_train_un))\n",
        "    scores_val = recall_score(y_val, model.predict(X_val))\n",
        "    print(\"Train Under Score: {}: {}\".format(name, scores))\n",
        "    print(\"Val Under Score: {}: {}\".format(name, scores_val))\n",
        "    # Create a temporary DataFrame for the new row\n",
        "    new_row_df = pd.DataFrame({'Model': [model] , 'ModelName' :[name +' UnderSampling'] , 'Train Recall': [scores], 'Validation Recall': [scores_val]})\n",
        "\n",
        "    # Concatenate the new row with the existing DataFrame\n",
        "    model_performance_df = pd.concat([model_performance_df, new_row_df], ignore_index=True)\n",
        "\n",
        "model_performance_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZGY1eL84jau"
      },
      "source": [
        "### HyperparameterTuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxM3jQuK_Pqc"
      },
      "source": [
        "#### Sample Parameter Grids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**\n",
        "\n",
        "1. Sample parameter grids have been provided to do necessary hyperparameter tuning. These sample grids are expected to provide a balance between model performance improvement and execution time. One can extend/reduce the parameter grid based on execution time and system configuration.\n",
        "  - Please note that if the parameter grid is extended to improve the model performance further, the execution time will increase\n"
      ],
      "metadata": {
        "id": "MSSmdBoHDJwO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czq7BZ5b4jau"
      },
      "source": [
        "- For Gradient Boosting:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    \"init\": [AdaBoostClassifier(random_state=1),DecisionTreeClassifier(random_state=1)],\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"subsample\":[0.7,0.9],\n",
        "    \"max_features\":[0.5,0.7,1],\n",
        "}\n",
        "```\n",
        "\n",
        "- For Adaboost:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"base_estimator\": [\n",
        "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ],\n",
        "}\n",
        "```\n",
        "\n",
        "- For Bagging Classifier:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    'max_samples': [0.8,0.9,1],\n",
        "    'max_features': [0.7,0.8,0.9],\n",
        "    'n_estimators' : [30,50,70],\n",
        "}\n",
        "```\n",
        "- For Random Forest:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50,110,25],\n",
        "    \"min_samples_leaf\": np.arange(1, 4),\n",
        "    \"max_features\": [np.arange(0.3, 0.6, 0.1),'sqrt'],\n",
        "    \"max_samples\": np.arange(0.4, 0.7, 0.1)\n",
        "}\n",
        "```\n",
        "\n",
        "- For Decision Trees:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    'max_depth': np.arange(2,6),\n",
        "    'min_samples_leaf': [1, 4, 7],\n",
        "    'max_leaf_nodes' : [10, 15],\n",
        "    'min_impurity_decrease': [0.0001,0.001]\n",
        "}\n",
        "```\n",
        "\n",
        "- For XGBoost (optional):\n",
        "\n",
        "```\n",
        "param_grid={'n_estimators':np.arange(50,110,25),\n",
        "            'scale_pos_weight':[1,2,5],\n",
        "            'learning_rate':[0.01,0.1,0.05],\n",
        "            'gamma':[1,3],\n",
        "            'subsample':[0.7,0.9]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tuning Decision Tree with UnderSampling data"
      ],
      "metadata": {
        "id": "jZsq5e-QXj_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model\n",
        "Model = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': np.arange(2,6),\n",
        "    'min_samples_leaf': [1, 4, 7],\n",
        "    'max_leaf_nodes' : [10, 15],\n",
        "    'min_impurity_decrease': [0.0001,0.001]\n",
        "}\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_un,y_train_un)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "zHFGDYtuXpcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new pipeline with best parameters\n",
        "tuned_dt = DecisionTreeClassifier(\n",
        "    random_state=1,\n",
        "    max_depth=5,\n",
        "    min_samples_leaf=7,\n",
        "    max_leaf_nodes=10,\n",
        "    min_impurity_decrease=0.0001\n",
        ")\n",
        "\n",
        "tuned_dt.fit(X_train, y_train)\n",
        "\n",
        "print(\"Train recall score : \" + str(recall_score(y_train, tuned_dt.predict(X_train))))\n",
        "print(\"Validation recall score : \"+ str(recall_score(y_val, tuned_dt.predict(X_val))))"
      ],
      "metadata": {
        "id": "HsUL-oI_XtRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMReRXdH_YUd"
      },
      "source": [
        "#### Tuning for XGBoost with original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9kks1hG_Xhy"
      },
      "outputs": [],
      "source": [
        "# defining XGBoost model\n",
        "Model = XGBClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid={'n_estimators':np.arange(50,110,25),\n",
        "            'scale_pos_weight':[1,2,5],\n",
        "            'learning_rate':[0.01,0.1,0.05],\n",
        "            'gamma':[1,3],\n",
        "            'subsample':[0.7,0.9]\n",
        "}\n",
        "\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train,y_train)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the XGBoost model with best parameters from above\n",
        "tuned_xgb = XGBClassifier(\n",
        "    random_state=1,\n",
        "    subsample=0.7,\n",
        "    scale_pos_weight=5,\n",
        "    n_estimators=75,\n",
        "    learning_rate=0.05,\n",
        "    gamma=3\n",
        ")\n",
        "\n",
        "tuned_xgb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Train recall score: \" + str(recall_score(y_train, tuned_xgb.predict(X_train))))\n",
        "print(\"Validation recall score: \" + str(recall_score(y_val, tuned_xgb.predict(X_val))))"
      ],
      "metadata": {
        "id": "Z9xElGsJQzTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chN8hbfThKyr"
      },
      "source": [
        "#### Tuning XG Boosting with oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVZcJ0hv4jau"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = XGBClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid={'n_estimators':np.arange(50,110,25),\n",
        "            'scale_pos_weight':[1,2,5],\n",
        "            'learning_rate':[0.01,0.1,0.05],\n",
        "            'gamma':[1,3],\n",
        "            'subsample':[0.7,0.9]\n",
        "}\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_over,y_train_over)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtPIiIS7hKyr"
      },
      "source": [
        "#### Creating the XGBoost model with best parameters from above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pbdykhHhKyr"
      },
      "outputs": [],
      "source": [
        "\n",
        "tuned_xgb1 = XGBClassifier(\n",
        "    random_state=1,\n",
        "    n_estimators=75,\n",
        "    scale_pos_weight=5,\n",
        "    learning_rate=0.05,\n",
        "    gamma=3,\n",
        "    subsample=0.9\n",
        ")\n",
        "\n",
        "tuned_xgb1.fit(X_train, y_train)\n",
        "\n",
        "print(\"Train recall score : \" + str(recall_score(y_train, tuned_xgb1.predict(X_train))))\n",
        "print(\"Validation recall score : \"+ str(recall_score(y_val, tuned_xgb1.predict(X_val))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tuning method for XGBoost with Undersampling data"
      ],
      "metadata": {
        "id": "hQrBkUvwQMTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model\n",
        "Model = XGBClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid={'n_estimators':np.arange(50,110,25),\n",
        "            'scale_pos_weight':[1,2,5],\n",
        "            'learning_rate':[0.01,0.1,0.05],\n",
        "            'gamma':[1,3],\n",
        "            'subsample':[0.7,0.9]\n",
        "}\n",
        "\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_un,y_train_un)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "qA-J9LFEQStG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new XGBoost with best tuned hyper parameters\n",
        "tuned_xgb2 = XGBClassifier(\n",
        "    random_state=1,\n",
        "    n_estimators= 75,\n",
        "    scale_pos_weight= 5,\n",
        "    subsample= 0.7,\n",
        "    learning_rate=0.05,\n",
        "    gamma=3\n",
        ")\n",
        "\n",
        "tuned_xgb2.fit(X_train, y_train)\n",
        "\n",
        "print(\"Train recall score: \"+ str(recall_score(y_train, tuned_xgb2.predict(X_train))))\n",
        "print(\"Validation recall score: \"+ str(recall_score(y_val, tuned_xgb2.predict(X_val))))"
      ],
      "metadata": {
        "id": "Vr7l1gxATpaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tuning Gradient Boosting with UnderSampling"
      ],
      "metadata": {
        "id": "PHG4O0DMUPGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model\n",
        "Model = GradientBoostingClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"init\": [AdaBoostClassifier(random_state=1),DecisionTreeClassifier(random_state=1)],\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"subsample\":[0.7,0.9],\n",
        "    \"max_features\":[0.5,0.7,1],\n",
        "}\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_un,y_train_un)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "iRUkG43_UOWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new Gradient Boosting with best hyper parameters\n",
        "tuned_gb = GradientBoostingClassifier(\n",
        "    random_state=1,\n",
        "    init= DecisionTreeClassifier(random_state=1),\n",
        "    n_estimators= 100,\n",
        "    max_features= 0.7,\n",
        "    subsample= 0.7,\n",
        "    learning_rate=0.05\n",
        ")\n",
        "\n",
        "tuned_gb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Train recall score: \"+ str(recall_score(y_train, tuned_gb.predict(X_train))))\n",
        "print(\"Validation recall score: \"+ str(recall_score(y_val, tuned_gb.predict(X_val))))"
      ],
      "metadata": {
        "id": "h4tKLMIwUXAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tuning AdaBoost with UnderSampling"
      ],
      "metadata": {
        "id": "vIKvIo49U1qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model\n",
        "Model = AdaBoostClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"estimator\": [\n",
        "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ],\n",
        "}\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_un,y_train_un)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "XDGHTF-RU3ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new AdaBoost model  with best hyper parameters\n",
        "tuned_ab = AdaBoostClassifier(\n",
        "    random_state=1,\n",
        "    n_estimators=75,\n",
        "    learning_rate=0.1,\n",
        "    estimator= DecisionTreeClassifier(max_depth=3, random_state=1)\n",
        ")\n",
        "\n",
        "tuned_ab.fit(X_train, y_train)\n",
        "\n",
        "print(\"Train recall score: \"+ str(recall_score(y_train, tuned_ab.predict(X_train))))\n",
        "print(\"Validation recall score: \"+ str(recall_score(y_val, tuned_ab.predict(X_val))))"
      ],
      "metadata": {
        "id": "Gt9BDkZYU7GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9JNnpxa4jau"
      },
      "source": [
        "## Test set final performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JG85rkY4jav"
      },
      "outputs": [],
      "source": [
        "# Predict the performance on the test set on all the tuned models\n",
        "print(\"Test Recall on tuned_xgb XGBoost \")\n",
        "print(recall_score(y_test, tuned_xgb.predict(X_test)))\n",
        "print(\"Test Recall on tuned_gb\")\n",
        "print(recall_score(y_test, tuned_gb.predict(X_test)))\n",
        "print(\"Test Recall on tuned_ab\")\n",
        "print(recall_score(y_test, tuned_ab.predict(X_test)))\n",
        "print(\"Test Recall on tuned_dt\")\n",
        "print(recall_score(y_test, tuned_dt.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derive the important features\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "feature_importances = tuned_xgb.feature_importances_\n",
        "\n",
        "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
        "\n",
        "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
        "\n",
        "\n",
        "imp_features = feature_importance_df.head(7)\n",
        "\n",
        "print()\n",
        "# Create a bar graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(imp_features['Feature'], imp_features['Importance'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top 7 Important Features for Tuned XGB')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to show most important feature at the top\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iW_z-OVUYlJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5hPmHyR4jaw"
      },
      "source": [
        "# Business Insights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When total_trans_ct declines, customers are more likely to attrite. Therefore, the business should engage with these customers and offer them attractive incentives to encourage retention.\n",
        "\n",
        "Similarly, if total_revolving_bal decreases, it indicates a higher propensity for attrition among customers, necessitating targeted offers to retain them.\n",
        "\n",
        "A decline in total_relationship_count also suggests that customers may be inclined to leave, reinforcing the need for the business to connect with these individuals and present them with compelling offers to retain their loyalty.\n",
        "\n",
        "Furthermore, if total_ct_chng_Q4_Q1 is falling, it signals an increased likelihood of attrition, prompting the business to proactively reach out to these customers with enticing offers.\n",
        "\n",
        "These factors are critical in understanding what influences customer attrition. To mitigate this risk, the marketing team should design campaigns aimed at different age groups, promoting suitable products for younger customers while emphasizing value for older ones."
      ],
      "metadata": {
        "id": "VAbGzDE2ZIdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "Based on these insights, the bank can develop targeted marketing campaigns aimed specifically at customers experiencing a decline in total transactions (total_trans_ct) and total revolving balance (total_revolving_bal). These campaigns should offer personalized incentives such as exclusive discounts, cashback on purchases, or loyalty rewards.\n"
      ],
      "metadata": {
        "id": "xrB_9Y3MZS-O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB3eO21n_sgt"
      },
      "source": [
        "***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9EaJ8AGwpM-2",
        "xxhpZv9y-qTw",
        "UvpMDcaaMKtI",
        "j-yGG8LNSSMa",
        "-YyWJgFlKlWM",
        "knk0w9XH4jao",
        "0J99-7Kubp09",
        "YZqmoqz7bp0-",
        "eqCDCbcw4jas",
        "oBKJaFU24jas",
        "1aimb6bn4jat",
        "yZGY1eL84jau",
        "rxM3jQuK_Pqc",
        "GMReRXdH_YUd",
        "chN8hbfThKyr",
        "HtPIiIS7hKyr",
        "D9JNnpxa4jau",
        "d_pDMFAz4jav",
        "c5hPmHyR4jaw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}